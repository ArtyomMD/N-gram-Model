{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 1: Text Suggestion\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 10 баллов. Сдавать задание после жесткого дедлайна нельзя. При сдачи решения после мягкого дедлайна за каждый день просрочки снимается по одному баллу.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "__Мягкий дедлайн: 24 нояб\n",
    "\n",
    "__Жесткий дедлайн: 27 нояб\n",
    "\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит реализовать систему, предлагающую удачное продолжение слова или нескольких следующих слов в режиме реального времени по типу тех, которые используются в телефонах, поисковой строке или приложении почты. Полученную систему вам нужно будет обернуть в пользовательский интерфейс с помощью библиотеки [reflex](https://github.com/reflex-dev/reflex), чтобы ей можно было удобно пользоваться, а так же, чтобы убедиться, что все работает как надо. В этот раз вам не придется обучать никаких моделей, мы ограничимся n-граммной генерацией.\n",
    "\n",
    "### Структура\n",
    "\n",
    "Это домашнее задание состоит из двух частей предположительно одинаковых по сложности. В первой вам нужно будет выполнить 5 заданий, по итогам которых вы получите минимально рабочее решение. А во второй, пользуясь тем, что вы уже сделали реализовать полноценную систему подсказки текста с пользовательским интерфейсом. Во второй части мы никак не будем ограничивать вашу фантазию. Делайте что угодно, лишь бы получилось в результате получился удобный фреймворк. Чем лучше у вас будет результат, тем больше баллов вы получите. Если будет совсем хорошо, то мы добавим бонусов сверху по своему усмотрению.\n",
    "\n",
    "### Оценивание\n",
    "При сдаче зададания в anytask вам будет необходимо сдать весь код, а также отчет с подробным описанием техник, которые в применили для создания вашей системы. Не лишним будет также написать и о том, что у вас не получилось и почему.\n",
    "\n",
    "За часть с заданиями можно будет получить до __5__ баллов, за отчет – до __3__ баллов, 2 балл за доп вопросы, если возникнут, если вопросов не возникло, считаем, что 2 балла вы получили "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "Для получения текстовых статистик используйте датасет `emails.csv`. Вы можете найти его по [ссылке](https://disk.yandex.ru/d/ikyUhWPlvfXxCg). Он содержит более 500 тысяч электронных писем на английском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517401"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "emails = pd.read_csv('emails.csv')\n",
    "len(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметьте, что данные очень грязные. В каждом письме содержится различная мета-информация, которая будет только мешать при предсказании продолжения текста.\n",
    "\n",
    "__Задание 1 (1 балл).__ Очистите корпус текстов по вашему усмотрению. В идеале обработанные тексты должны содержать только текст самого письма и ничего лишнего по типу ссылок, адресатов и прочих символов, которыми мы точно не хотим продолжать текст. Оценка будет выставляться по близости вашего результата к этому идеалу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracy, is there some particular reason I'm getting this notice? Debra Perlingiere is the paralegal that does the Master Firm Purchase/Sale Agreements.\n",
      "['Tracy', ',', 'is', 'there', 'some', 'particular', 'reason', 'I', \"'\", 'm', 'getting', 'this', 'notice', '?', 'Debra', 'Perlingiere', 'is', 'the', 'paralegal', 'that', 'does', 'the', 'Master', 'Firm', 'Purchase', '/', 'Sale', 'Agreements', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def clean_email(content):\n",
    "    # Если строка начинается с \"To:\", удаляем её и две предыдущие строки\n",
    "    lines = content.split('\\n')\n",
    "    stop_list = []\n",
    "    for i in range(len(lines)):  \n",
    "        # Если строка начинается с \"To:\", удаляем её и две предыдущие строки\n",
    "        if lines[-i-1].strip().startswith(\"To:\"):\n",
    "            stop_list.append(len(lines)-i-1)\n",
    "            stop_list.append(len(lines)-i-2)\n",
    "            stop_list.append(len(lines)-i-3)\n",
    "    stop_list= sorted(list(set(stop_list)))\n",
    "    clean_lines = [elem for i, elem in enumerate(lines) if i not in stop_list]\n",
    "\n",
    "    # Собираем текст обратно в одну строку\n",
    "    content = \"\\n\".join(clean_lines).strip()\n",
    "    \n",
    "    # Удаляем строки с метаданными (Message-ID, Date, From, To, Subject и другие)\n",
    "    content = re.sub(r\"(?i)(Message-ID|Date|From|Subject|Mime-Version|Content-Type|cc|Content-Transfer-Encoding|X-From|X-To|X-cc|X-bcc|X-Folder|X-Origin|X-FileName):.*\", \"\", content)\n",
    "    content = re.sub(r\"--\", \"\", content)\n",
    "    content = re.sub(r\"==\", \"\", content)\n",
    "    content = re.sub(r\"__\", \"\", content)\n",
    "    content = re.sub(r\"\\*{2}\", \"\", content)\n",
    "    content = re.sub(r\"IMAGE\", \"\", content)\n",
    "    \n",
    "    # Убираем строку с «Forwarded by» и подобные повторяющиеся сообщения\n",
    "    content = re.sub(r\"(?i)^(.*?Forwarded by.*?)$\", \"\", content, flags=re.MULTILINE)\n",
    "\n",
    "    # Удаляем строки с символом '@'\n",
    "    content = re.sub(r\"^.*@.*$\", \"\", content, flags=re.MULTILINE)\n",
    "\n",
    "    # Убираем ненужные пустые строки (состоящие только из пробелов)\n",
    "    content = re.sub(r\"^\\s*$\", \"\", content, flags=re.MULTILINE)\n",
    "\n",
    "    # Убираем все переносы строк, заменяя их на пробелы\n",
    "    content = re.sub(r\"\\n\", \" \", content)\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "\n",
    "    if len(content) == 0:\n",
    "        return None\n",
    "\n",
    "    if content[0] == ' ':\n",
    "        content = content[1:]\n",
    "\n",
    "    return content\n",
    "\n",
    "email_text = emails['message'][random.randint(0, 517401)]\n",
    "cleaned_email = clean_email(email_text)\n",
    "print(cleaned_email)\n",
    "\n",
    "tokens = re.findall(r'\\w+|[^\\w\\s]', cleaned_email, re.UNICODE)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для следующего задания вам нужно будет токенизировать текст. Для этого просто разбейте его по словам. Очевидно, итоговый результат будет лучше, если ваша система также будет предлагать уместную пунктуацию. Но если вы считаете, что результат получается лучше без нее, то можете удалить все небуквенные символы на этапе токенизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнение слова\n",
    "\n",
    "Описанная система будет состоять из двух частей: дополнение слова до целого и генерация продолжения текста (или вариантов продолжений). Начнем с первой части.\n",
    "\n",
    "В этой части вам предстоит реализовать метод дополнения слова до целого по его началу (префиксу). Для этого сперва необходимо научиться находить все слова, имеющие определенный префикс. Мы будем вызывать функцию поиска подходящих слов после каждой напечатанной пользователем буквы. Поэтому нам очень важно, чтобы поиск работал как можно быстрее. Простой перебор всех слов занимает $O(|V| \\cdot n)$ времени, где $|V|$ – размер словаря, а $n$ – длина префикса. Мы же напишем [префиксное дерево](https://ru.wikipedia.org/wiki/Префиксное_дерево), которое позволяет искать слова за $O(n + m)$, где $m$ – число подходящих слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2 (1 балл).__ Допишите префиксное дерево для поиска слов по префиксу. Ваше дерево должно работать за $O(n + m)$ операции, в противном случае вы не получите баллов за это задание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, defaultdict\n",
    "\n",
    "class PrefixTreeNode:\n",
    "    def __init__(self):\n",
    "        # словарь с буквами, которые могут идти после данной вершины\n",
    "        self.children: dict[str, PrefixTreeNode] = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "class PrefixTree:\n",
    "    def __init__(self, vocabulary: List[str]):\n",
    "        \"\"\"\n",
    "        vocabulary: список всех уникальных токенов в корпусе\n",
    "        \"\"\"\n",
    "        self.root = PrefixTreeNode()\n",
    "        \n",
    "        for word in tqdm(vocabulary):\n",
    "            current_node = self.root\n",
    "            for letter in word:\n",
    "                if letter not in current_node.children:\n",
    "                    current_node.children[letter] = PrefixTreeNode()\n",
    "                current_node = current_node.children[letter]\n",
    "            current_node.is_end_of_word = True\n",
    "\n",
    "    def search_prefix(self, prefix) -> List[str]:\n",
    "        \"\"\"\n",
    "        Возвращает все слова, начинающиеся на prefix\n",
    "        prefix: str – префикс слова\n",
    "        \"\"\"\n",
    "\n",
    "        current_node = self.root\n",
    "        for letter in prefix:\n",
    "            if letter not in current_node.children:\n",
    "                return []  # Если префикс не найден\n",
    "            current_node = current_node.children[letter]\n",
    "            \n",
    "        result = []\n",
    "        self._find_words_with_prefix(current_node, prefix, result)\n",
    "        return result\n",
    "\n",
    "    def _find_words_with_prefix(self, node: PrefixTreeNode, prefix: str, result: List[str]):\n",
    "        \"\"\"\n",
    "        Рекурсивно находим все слова от текущей вершины\n",
    "        \"\"\"\n",
    "        if node.is_end_of_word:\n",
    "            result.append(prefix)\n",
    "        \n",
    "        # Проходим по всем дочерним вершинам\n",
    "        for letter, child_node in node.children.items():\n",
    "            self._find_words_with_prefix(child_node, prefix + letter, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = ['aa', 'aaa', 'abb', 'bba', 'bbb', 'bcd']\n",
    "prefix_tree = PrefixTree(vocabulary)\n",
    "\n",
    "assert set(prefix_tree.search_prefix('a')) == set(['aa', 'aaa', 'abb'])\n",
    "assert set(prefix_tree.search_prefix('bb')) == set(['bba', 'bbb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть способ быстро находить все слова с определенным префиксом, нам нужно их упорядочить по вероятности, чтобы выбирать лучшее. Будем оценивать вероятность слова по частоте его встречаемости в корпусе.\n",
    "\n",
    "__Задание 3 (1 балл).__ Допишите класс `WordCompletor`, который формирует словарь и префиксное дерево, а так же умеет находить все возможные продолжения слова вместе с их вероятностями. В этом классе вы можете при необходимости дополнительно отфильтровать слова, например, удалив все самые редкие. Постарайтесь максимально оптимизировать ваш код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import defaultdict\n",
    "\n",
    "class WordCompletor:\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        corpus: list – корпус текстов\n",
    "        \"\"\"\n",
    "        self.volume, self.word_counts = self._get_word_counts(corpus) # Определяем общий объём слов и количество каждого уникального слова\n",
    "        self.prefix_tree = PrefixTree(list(self.word_counts.keys()))\n",
    "\n",
    "    def get_words_and_probs(self, prefix: str) -> (List[str], List[float]):\n",
    "        \"\"\"\n",
    "        Возвращает список слов, начинающихся на prefix,\n",
    "        с их вероятностями (нормировать ничего не нужно)\n",
    "        \"\"\"\n",
    "        words = self.prefix_tree.search_prefix(prefix) # Слова, начинающиеся на префикс\n",
    "        probs = [self.word_counts[i]/self.volume for i in words] # Вероятность = количество слова / общее количесвто слов\n",
    "        \n",
    "        return words, probs\n",
    "\n",
    "    def _get_word_counts(self, corpus: List[List[str]]) -> dict:\n",
    "        word_counts = defaultdict(int)\n",
    "        volume = 0\n",
    "        # Подсчитываем частоту появления слов в списках\n",
    "        for doc in tqdm(corpus):\n",
    "            seen_words = set()  # Чтобы слово не учитывалось несколько раз в одном документе\n",
    "            volume += len(doc)\n",
    "            for word in doc:\n",
    "                if word not in seen_words:\n",
    "                    word_counts[word] += 1\n",
    "                    seen_words.add(word)\n",
    "        return volume, word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████| 8/8 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "dummy_corpus = [\n",
    "    [\"aa\", \"ab\"],\n",
    "    [\"aaa\", \"abab\"],\n",
    "    [\"abb\", \"aa\", \"ab\", \"bba\", \"bbb\", \"bcd\"],\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "words, probs = word_completor.get_words_and_probs('a')\n",
    "words_probs = list(zip(words, probs))\n",
    "assert set(words_probs) == {('aa', 0.2), ('ab', 0.2), ('aaa', 0.1), ('abab', 0.1), ('abb', 0.1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание следующих слов\n",
    "\n",
    "Теперь, когда мы умеем дописывать слово за пользователем, мы можем пойти дальше и предожить ему несколько следующих слов с учетом дописанного. Для этого мы воспользуемся n-граммами и будем советовать n следующих слов. Но сперва нужно получить n-граммную модель.\n",
    "\n",
    "Напомним, что вероятность последовательности для такой модели записывается по формуле\n",
    "$$\n",
    "P(w_1, \\dots, w_T) = \\prod_{i=1}^T P(w_i \\mid w_{i-1}, \\dots, w_{i-n}).\n",
    "$$\n",
    "\n",
    "Тогда, нам нужно оценить $P(w_i \\mid w_{i-1}, \\dots, w_{i-n})$ по частоте встречаемости n-граммы.   \n",
    "\n",
    "__Задание 4 (1 балл).__ Напишите класс для n-граммной модели. Понятное дело, никакого сглаживания добавлять не надо, мы же не хотим, чтобы модель советовала случайные слова (хоть и очень редко)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        # Словарь для хранения n-грамм: {префикс -> {следующее слово: частота}}\n",
    "        self.ngram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.prefix_counts = defaultdict(int)\n",
    "        \n",
    "        # Строим модель на основе корпуса\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Строит n-граммную модель на основе корпуса.\n",
    "        \"\"\"\n",
    "        for sentence in tqdm(self.corpus):\n",
    "            # Обрабатываем каждое предложение\n",
    "            for i in range(len(sentence) - self.n):\n",
    "                # Берем n-грамму из предложения\n",
    "                prefix = tuple(sentence[i:i + self.n])  # (n-1)-грамма\n",
    "                next_word = sentence[i + self.n]  # Следующее слово\n",
    "                \n",
    "                # Обновляем счетчик для префикса\n",
    "                self.ngram_counts[prefix][next_word] += 1\n",
    "                self.prefix_counts[prefix] += 1\n",
    "\n",
    "    def get_next_words_and_probs(self, prefix: list) -> (List[str], List[float]):\n",
    "        \"\"\"\n",
    "        Возвращает список слов, которые могут идти после prefix,\n",
    "        а также список вероятностей этих слов.\n",
    "        \"\"\"\n",
    "        flag=0\n",
    "        old_n = self.n\n",
    "        if len(prefix) > self.n:\n",
    "            prefix = prefix[-self.n:]\n",
    "        elif len(prefix) < self.n:\n",
    "            self.n = len(prefix)\n",
    "            self._build_model()\n",
    "            flag=1\n",
    "            \n",
    "        # Преобразуем prefix в кортеж\n",
    "        prefix_tuple = tuple(prefix)\n",
    "        \n",
    "        # Получаем возможные следующие слова\n",
    "        next_words = list(self.ngram_counts[prefix_tuple].keys())\n",
    "        \n",
    "        # Вычисляем вероятности\n",
    "        probs = [\n",
    "            count / self.prefix_counts[prefix_tuple]\n",
    "            for count in self.ngram_counts[prefix_tuple].values()\n",
    "        ]\n",
    "\n",
    "        if flag:\n",
    "            self.n = old_n\n",
    "            self._build_model()\n",
    "            \n",
    "        return next_words, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "\n",
    "next_words, probs = n_gram_model.get_next_words_and_probs(['aa', 'aa'])\n",
    "words_probs = list(zip(next_words, probs))\n",
    "set(words_probs)\n",
    "\n",
    "assert set(words_probs) == {('aa', 2/3), ('ab', 1/3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы теперь можем объединить два метода в автоматический дописыватель текстов: первый будет дополнять слово, а второй – предлагать продолжения. Хочется, чтобы предлагался список возможных продолжений, из который пользователь сможет выбрать наиболее подходящее. Самое сложное тут – аккуратно выбирать, что показывать, а что нет.   \n",
    "\n",
    "__Задание 5 (1 балл).__ В качестве первого подхода к снаряду реализуйте метод, возвращающий всегда самое вероятное продолжение жадным способом. Если вы справитесь, то сможете можете добавить опцию поддержки нескольких вариантов продолжений, что сделает метод гораздо лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import random\n",
    "\n",
    "class TextSuggestion:\n",
    "    def __init__(self, word_completor, n_gram_model):\n",
    "        self.word_completor = word_completor\n",
    "        self.n_gram_model = n_gram_model\n",
    "        \n",
    "    def inp2key(self, inp, l=10):\n",
    "        k = {i:j for i,j in zip(*self.n_gram_model.get_next_words_and_probs(inp))}\n",
    "        top_l = dict(sorted(k.items(), key=lambda item: item[1], reverse=True)[:l])\n",
    "        return top_l\n",
    "        \n",
    "    def get_tree(self, inp, ns):\n",
    "        f = self.inp2key(inp)\n",
    "        s = {}\n",
    "        for k in f.keys():\n",
    "            s0 = self.inp2key(inp[1:]+[k])\n",
    "            s0 = {tuple([k]+[key]): value*f[k] for key, value in s0.items()}\n",
    "            s = s|s0\n",
    "        for i in range(ns-2):\n",
    "            f = s.copy()\n",
    "            s = {}\n",
    "            for k in f.keys():\n",
    "                s0 = self.inp2key(tuple(inp[1:]+list(k)))\n",
    "                s0 = {tuple(list(k)+[key]): value*f[k] for key, value in s0.items()}\n",
    "                s = s|s0\n",
    "        return s\n",
    "        \n",
    "    def suggest_text(self, text: list, n_words=3, n_texts=1) -> list[list[str]]:\n",
    "        \"\"\"\n",
    "        Возвращает возможные варианты продолжения текста (по умолчанию только один)\n",
    "        \n",
    "        text: строка или список слов – написанный пользователем текст\n",
    "        n_words: число слов, которые дописывает n-граммная модель\n",
    "        n_texts: число возвращаемых продолжений (пока что только одно)\n",
    "        \n",
    "        return: list[list[srt]] – список из n_texts списков слов, по 1 + n_words слов в каждом\n",
    "        Первое слово – это то, которое WordCompletor дополнил до целого.\n",
    "        \"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        words, probs = self.word_completor.get_words_and_probs(text[-1])\n",
    "        if len(probs) == 0:\n",
    "            return [[]]\n",
    "        else:\n",
    "            first = words[probs.index(max(probs))]\n",
    "\n",
    "        out = []\n",
    "        inp = text[:-1]+[first]\n",
    "        for i in range(n_words):\n",
    "            words, probs = self.n_gram_model.get_next_words_and_probs(inp)\n",
    "            if len(probs) == 0:\n",
    "                return [[]]\n",
    "            else:\n",
    "                pred = words[probs.index(max(probs))]\n",
    "                out.append(pred)\n",
    "                inp.append(pred)\n",
    "\n",
    "        most = inp[-n_words-1:]\n",
    "        suggestions.append(list(most))\n",
    "        if n_texts > 1:\n",
    "            sp = list(self.get_tree(inp, n_words).keys())\n",
    "            sp = [[first]+list(t) for t in sp]\n",
    "            if most in sp:\n",
    "                sp.remove(most)\n",
    "            for t in random.sample(sp, min(len(sp), n_texts-1)):\n",
    "                suggestions.append(t)\n",
    "        \n",
    "        return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 3/3 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████| 8/8 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████| 3/3 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)\n",
    "\n",
    "assert text_suggestion.suggest_text(['aa', 'aa'], n_words=3, n_texts=1) == [['aa', 'aa', 'aa', 'aa']]\n",
    "assert text_suggestion.suggest_text(['abb', 'aa', 'ab'], n_words=2, n_texts=1) == [['ab', 'bba', 'bbb']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время довести вашу систему до ума. В этой части вы можете модифицировать все классы по своему усмотрению и добавлять любые эвристики. Если нужно, то дополнительно обрабатывать текст и вообще делать все, что считаете нужным, __кроме использования дополнительных данных__. Главное – вы должны обернуть вашу систему в пользовательский интерфейс с помощью [reflex](https://github.com/reflex-dev/reflex). В нем можно реализовать почти любой функционал по вашему желанию.\n",
    "\n",
    "Мы настоятельно рекомендуем вам оформить код в проект, а не писать в ноутбуке. Но если вам очень хочется писать тут, то хотя бы не меняйте код в предыдущих заданиях, чтобы его можно было нормально оценивать.\n",
    "\n",
    "При сдаче решения прикрепите весь ваш __код__, __отчет__ по второй части и __видео__ с демонстрацией работы вашей системы. Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 2349.79it/s]\n"
     ]
    }
   ],
   "source": [
    "full_corpus = []\n",
    "for i in tqdm(range(len(emails)):\n",
    "    email_text = emails['message'][i]\n",
    "    cleaned_email = clean_email(email_text)\n",
    "    if cleaned_email is not None:\n",
    "        cleaned_email = re.sub(r\"[^a-zA-Z.!? ]+\", \"\", cleaned_email.replace(\"!\", \".\").replace(\"?\", \".\"))\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', cleaned_email.lower(), re.UNICODE)\n",
    "        if '@' in tokens:\n",
    "            print('@')\n",
    "        full_corpus.append(['@','@']+tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 99/99 [00:00<00:00, 33010.26it/s]\n",
      "100%|██████████████████████████████████| 2241/2241 [00:00<00:00, 373749.86it/s]\n"
     ]
    }
   ],
   "source": [
    "word_completor = WordCompletor(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 99/99 [00:00<00:00, 5500.69it/s]\n"
     ]
    }
   ],
   "source": [
    "n_gram_model = NGramLanguageModel(corpus=full_corpus, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = text_suggestion.suggest_text(['here', 'is', 'm'], n_words=4, n_texts=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
